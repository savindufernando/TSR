{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "intro",
      "metadata": {},
      "source": [
        "# Hybrid CNN + ViT Traffic Sign Recognition\n",
        "End-to-end notebook: dataset download/selection, dataloaders, training, eval, TFLite export.\n",
        "**Set EPOCHS higher (e.g., 50) for full training.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "install",
      "metadata": {},
      "source": [
        "## 1) Install dependencies\n",
        "Uncomment the pip line if running in a clean environment. TensorFlow needs Python 3.10-3.12.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pip",
      "metadata": {},
      "outputs": [],
      "source": [
        "# If needed, install requirements.\n",
        "# !pip install -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "data_setup",
      "metadata": {},
      "source": [
        "## 2) Locate or download the GTSRB dataset\n",
        "Uses data/dataset_path.txt if present (written by scripts/download_dataset.py). Falls back to KaggleHub download.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dataset",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "DATASET_POINTER = Path(\"data/dataset_path.txt\")\n",
        "if DATASET_POINTER.exists():\n",
        "    dataset_root = Path(DATASET_POINTER.read_text().strip())\n",
        "    print(f\"Using dataset from pointer: {dataset_root}\")\n",
        "else:\n",
        "    print(\"dataset_path.txt not found; attempting KaggleHub download (requires Kaggle auth).\")\n",
        "    import kagglehub\n",
        "\n",
        "    dataset_root = Path(kagglehub.dataset_download(\"meowmeowmeowmeowmeow/gtsrb-german-traffic-sign\"))\n",
        "    DATASET_POINTER.parent.mkdir(parents=True, exist_ok=True)\n",
        "    DATASET_POINTER.write_text(str(dataset_root), encoding=\"utf-8\")\n",
        "    print(f\"Wrote pointer to {DATASET_POINTER}\")\n",
        "\n",
        "if not dataset_root.exists():\n",
        "    raise FileNotFoundError(f\"Dataset path does not exist: {dataset_root}\")\n",
        "\n",
        "train_dir = dataset_root / \"Train\"\n",
        "print(f\"Train dir: {train_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "explore",
      "metadata": {},
      "source": [
        "## 3) Quick dataset sanity check\n",
        "Counts a few class folders and confirms test presence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "inspect",
      "metadata": {},
      "outputs": [],
      "source": [
        "class_dirs = [p for p in train_dir.iterdir() if p.is_dir()]\n",
        "print(f\"Found {len(class_dirs)} classes\")\n",
        "sample_counts = {p.name: len(list(p.glob('*'))) for p in class_dirs[:5]}\n",
        "print(\"Sample class counts (first 5):\", sample_counts)\n",
        "test_dir = dataset_root / \"Test\"\n",
        "print(\"Has Test directory:\", test_dir.exists())\n",
        "print(\"Has Test.csv:\", (dataset_root / 'Test.csv').exists() or (dataset_root / 'test.csv').exists())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dataloaders_md",
      "metadata": {},
      "source": [
        "## 4) Build TensorFlow dataloaders\n",
        "Augmentation + normalization are applied inside the pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dataloaders",
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tsr.data import DatasetConfig, load_gtsrb_datasets\n",
        "\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "data_cfg = DatasetConfig(img_size=IMG_SIZE, batch_size=BATCH_SIZE)\n",
        "train_ds, val_ds, test_ds, num_classes = load_gtsrb_datasets(dataset_root, data_cfg)\n",
        "print(\"num_classes:\", num_classes)\n",
        "for batch_x, batch_y in train_ds.take(1):\n",
        "    print(\"Batch shapes:\", batch_x.shape, batch_y.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "model_md",
      "metadata": {},
      "source": [
        "## 5) Build hybrid CNN + Transformer model\n",
        "MobileNetV2 backbone + transformer encoder over feature-map tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "model",
      "metadata": {},
      "outputs": [],
      "source": [
        "from tsr.model import ModelConfig, build_hybrid_cnn_vit\n",
        "\n",
        "model_cfg = ModelConfig(img_size=IMG_SIZE, num_classes=num_classes, backbone_trainable=False)\n",
        "model = build_hybrid_cnn_vit(model_cfg)\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=3e-4),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\n",
        "        \"accuracy\",\n",
        "        tf.keras.metrics.Precision(name=\"precision\"),\n",
        "        tf.keras.metrics.Recall(name=\"recall\"),\n",
        "    ],\n",
        ")\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "train_md",
      "metadata": {},
      "source": [
        "## 6) Train\n",
        "Set EPOCHS higher (e.g., 50) for best accuracy; a small number keeps the demo quick.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "train",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "EPOCHS = 3  # increase for real training\n",
        "OUT_DIR = Path(\"outputs\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "(OUT_DIR / \"logs\").mkdir(parents=True, exist_ok=True)\n",
        "(OUT_DIR / \"checkpoints\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2),\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=str(OUT_DIR / \"checkpoints\" / \"best.keras\"),\n",
        "        monitor=\"val_loss\",\n",
        "        save_best_only=True,\n",
        "    ),\n",
        "    tf.keras.callbacks.TensorBoard(log_dir=str(OUT_DIR / \"logs\")),\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=callbacks,\n",
        ")\n",
        "\n",
        "saved_model_dir = OUT_DIR / \"saved_model\"\n",
        "model.save(saved_model_dir)\n",
        "print(\"SavedModel:\", saved_model_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eval_md",
      "metadata": {},
      "source": [
        "## 7) Evaluate on test set (if present)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eval",
      "metadata": {},
      "outputs": [],
      "source": [
        "if test_ds is not None:\n",
        "    eval_results = model.evaluate(test_ds, verbose=2)\n",
        "    metrics = dict(zip(model.metrics_names, eval_results))\n",
        "    print(\"Test metrics:\", metrics)\n",
        "else:\n",
        "    print(\"No test set detected; skipping test evaluation.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tflite_md",
      "metadata": {},
      "source": [
        "## 8) Export TensorFlow Lite\n",
        "Adjust the block for full INT8 quantization (needs representative data).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tflite",
      "metadata": {},
      "outputs": [],
      "source": [
        "converter = tf.lite.TFLiteConverter.from_saved_model(str(saved_model_dir))\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "# For full int8: set representative_dataset using load_gtsrb_datasets(..., apply_preprocessing=False)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "tflite_path = OUT_DIR / \"model.tflite\"\n",
        "tflite_path.write_bytes(tflite_model)\n",
        "print(\"TFLite model written to:\", tflite_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "next",
      "metadata": {},
      "source": [
        "## Next steps\n",
        "- Increase `EPOCHS`, set `backbone_trainable=True` for fine-tuning, and monitor `outputs/logs` in TensorBoard.\n",
        "- For full INT8 export, pass `int8=True` in scripts/export_tflite.py or set a representative dataset here.\n",
        "- Pair this classifier with a detector/ROI cropper for real-world traffic scenes.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
